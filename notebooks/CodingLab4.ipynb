{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Neural Data Science_\n",
    "\n",
    "Lecturer: Dr. Jan Lause, Prof. Dr. Philipp Berens\n",
    "\n",
    "Tutors: Jonas Beck, Fabio Seel, Julius Würzler\n",
    "\n",
    "Summer term 2025\n",
    "\n",
    "Student names: <span style='background: yellow'>*FILL IN YOUR NAMES HERE* </span>\n",
    "\n",
    "LLM Disclaimer: <span style='background: yellow'>*Did you use an LLM to solve this exercise? If yes, which one and where did you use it? [Copilot, Claude, ChatGPT, etc.]* </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Lab 4\n",
    "\n",
    "In this notebook you will work with preprocessed 2 photon calcium recordings, that have already been converted into spike counts for a population of cells from the Macaque V1. During the experiment the animal has been presented with several drifting grating stimuli, in response to which the neural activity was recorded. In this exercise sheet we will study how you can visualize the activity of multiple neural spike trains and assess whether a neuron is selective to a specific stimulus type.\n",
    "\n",
    "Download the data files ```nds_cl_4_*.csv``` from ILIAS and save it in the subfolder ```../data/```. We recommend you to use a subset of the data for testing and debugging, ideally focus on a single cell (e.g. cell number x). The spike times and stimulus conditions are read in as pandas data frames. You can solve the exercise by making heavy use of that, allowing for many quite compact computations. See [documentation](http://pandas.pydata.org/pandas-docs/stable/index.html) and several good [tutorials](https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python#gs.L37i87A) on how to do this. Of course, converting the data into classical numpy arrays is also valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "\n",
    "from scipy import signal as signal\n",
    "from typing import Tuple\n",
    "\n",
    "import itertools\n",
    "import logging\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#%load_ext jupyter_black\n",
    "\n",
    "#%load_ext watermark\n",
    "#%watermark --time --date --timezone --updated --python --iversions --watermark -p sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 10:52:32,056 - INFO - Starting the script...\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    level=logging.DEBUG,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info(\"Starting the script...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"../matplotlib_style.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes = pd.read_csv(\"../data/nds_cl_4_spiketimes.csv\")  # neuron id, spike time\n",
    "stims = pd.read_csv(\"../data/nds_cl_4_stimulus.csv\")  # stimulus onset in ms, direction\n",
    "\n",
    "stimDur = 2000.0  # in ms\n",
    "nTrials = 11  # number of trials per condition\n",
    "nDirs = 16  # number of conditions\n",
    "deltaDir = 22.5  # difference between conditions\n",
    "\n",
    "stims[\"StimOffset\"] = stims[\"StimOnset\"] + stimDur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We require some more information about the spikes for the plots and analyses we intend to make later. With a solution based on dataframes, it is natural to compute this information here and add it as additional columns to the `spikes` dataframe by combining it with the `stims` dataframe. We later need to know which condition (`Dir`) and trial (`Trial`) a spike was recorded in, the relative spike times compared to stimulus onset of the stimulus it was recorded in (`relTime`) and whether a spike was during the stimulation period (`stimPeriod`). But there are many options how to solve this exercise and you are free to choose any of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_q/9l1zk1853mv45phss2fsffd00000gn/T/ipykernel_30883/4012369867.py:21: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'True' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  spikes.loc[select, \"stimPeriod\"] = True\n"
     ]
    }
   ],
   "source": [
    "# you may add computations as specified above\n",
    "spikes[\"Dir\"] = np.nan\n",
    "spikes[\"relTime\"] = np.nan\n",
    "spikes[\"Trial\"] = np.nan\n",
    "spikes[\"stimPeriod\"] = np.nan\n",
    "\n",
    "dirs = np.unique(stims[\"Dir\"])\n",
    "trialcounter = np.zeros_like(dirs)\n",
    "\n",
    "for i, row in stims.iterrows():\n",
    "    trialcounter[dirs == row[\"Dir\"]] += 1\n",
    "\n",
    "    i0 = spikes[\"SpikeTimes\"] > row[\"StimOnset\"]\n",
    "    i1 = spikes[\"SpikeTimes\"] < row[\"StimOffset\"]\n",
    "\n",
    "    select = i0.values & i1.values\n",
    "\n",
    "    spikes.loc[select, \"Dir\"] = row[\"Dir\"]\n",
    "    spikes.loc[select, \"Trial\"] = trialcounter[dirs == row[\"Dir\"]][0]\n",
    "    spikes.loc[select, \"relTime\"] = spikes.loc[select, \"SpikeTimes\"] - row[\"StimOnset\"]\n",
    "    spikes.loc[select, \"stimPeriod\"] = True\n",
    "\n",
    "spikes = spikes.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Neuron</th>\n",
       "      <th>SpikeTimes</th>\n",
       "      <th>Dir</th>\n",
       "      <th>relTime</th>\n",
       "      <th>Trial</th>\n",
       "      <th>stimPeriod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>1</td>\n",
       "      <td>15739.000000</td>\n",
       "      <td>270.0</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>1</td>\n",
       "      <td>15776.566667</td>\n",
       "      <td>270.0</td>\n",
       "      <td>206.566667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>1</td>\n",
       "      <td>15808.466667</td>\n",
       "      <td>270.0</td>\n",
       "      <td>238.466667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>1</td>\n",
       "      <td>15821.900000</td>\n",
       "      <td>270.0</td>\n",
       "      <td>251.900000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>1</td>\n",
       "      <td>15842.966667</td>\n",
       "      <td>270.0</td>\n",
       "      <td>272.966667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Neuron    SpikeTimes    Dir     relTime  Trial stimPeriod\n",
       "514       1  15739.000000  270.0  169.000000    1.0       True\n",
       "515       1  15776.566667  270.0  206.566667    1.0       True\n",
       "516       1  15808.466667  270.0  238.466667    1.0       True\n",
       "517       1  15821.900000  270.0  251.900000    1.0       True\n",
       "518       1  15842.966667  270.0  272.966667    1.0       True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spikes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Plot spike rasters\n",
    "\n",
    "In a raster plot, each spike is shown by a small tick at the time it occurs relative to stimulus onset. Implement a function `plotRaster()` that plots the spikes of one cell as one trial per row, sorted by conditions (similar to what you saw in the lecture). Why are there no spikes in some conditions and many in others?\n",
    "\n",
    "If you opt for a solution without a dataframe, you need to change the interface of the function.\n",
    "\n",
    "*Grading: 3 pts*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotRaster(spikes: pd.DataFrame, neuron: int):\n",
    "    \"\"\"plot spike rasters for a single neuron sorted by condition\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    spikes: pd.DataFrame\n",
    "        Pandas DataFrame with columns\n",
    "            Neuron | SpikeTimes | Dir | relTime | Trial | stimPeriod\n",
    "\n",
    "    neuron: int\n",
    "        Neuron ID\n",
    "\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "\n",
    "    this function does not return anything, it just creates a plot!\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Write a raster plot function for the data (2 pts)\n",
    "    # -------------------------------------------------\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find examples of \n",
    "1. a direction selective neuron\n",
    "2. an orientation selective neuron \n",
    "3. neither\n",
    "\n",
    "and explain your choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# Find and explain examples? (1 pt)\n",
    "# ---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Plot spike density functions\n",
    "\n",
    "Compute an estimate of the spike rate against time relative to stimulus onset. There are two ways:\n",
    "* Discretize time: Decide on a bin size, count the spikes in each bin and average across trials. \n",
    "* Directly estimate the probability of spiking using a density estimator with specified kernel width. \n",
    "\n",
    "For full points, the optimal kernel- or bin-width needs to be computed.\n",
    "\n",
    "Implement one of them in the function `plotPSTH()`. If you dont use a dataframe you may need to change the interface of the function.\n",
    "\n",
    "\n",
    "*Grading: 4 pts*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPSTH(spikes: pd.DataFrame, neuron: int):\n",
    "    \"\"\"Plot PSTH for a single neuron sorted by condition\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    spikes: pd.DataFrame\n",
    "        Pandas DataFrame with columns\n",
    "            Neuron | SpikeTimes | Dir | relTime | Trial | stimPeriod\n",
    "\n",
    "    neuron: int\n",
    "        Neuron ID\n",
    "\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "\n",
    "    this function does not return anything, it just creates a plot!\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Implement one of the spike rate estimates (3 pts)\n",
    "    # -------------------------------------------------\n",
    "\n",
    "    for row, dir in enumerate(dirs):\n",
    "        # ---------------------------------------------\n",
    "        # Plot the obtained spike rate estimates (1 pt)\n",
    "        # ---------------------------------------------\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the same 3 examples you selected in Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Fit and plot tuning functions\n",
    "\n",
    "The goal is to visualize the activity of each neuron as a function of stimulus direction. First, compute the spike counts of each neuron for each direction of motion and trial.  The result should be a matrix `x`, where $x_{jk}$ represents the spike count of the $j$-th response to the $k$-th direction of motion (i.e. each column contains the spike counts for all trials with one direction of motion).\tIf you used dataframes above, the `groupby()` function allows to implement this very compactly. Make sure you don't loose trials with zero spikes though. Again, other implementations are completely fine.\n",
    "\n",
    "Fit the tuning curve, i.e. the average spike count per direction, using a von Mises model. To capture the non-linearity and direction selectivity of the neurons, we will fit a modified von Mises function:\n",
    "\n",
    "$$ f(\\theta) = \\exp(\\alpha + \\kappa (\\cos (2*(\\theta-\\phi))-1) + \\nu (\\cos (\\theta-\\phi)-1))$$\n",
    "\n",
    "Here, $\\theta$ is the stimulus direction. Implement the von Mises function in `vonMises()` and plot it to understand how to interpret its parameters $\\phi$, $\\kappa$, $\\nu$, $\\alpha$. Perform a non-linear least squares fit using a package/function of your choice. Implement the fitting in `tuningCurve()`. \n",
    "\n",
    "Plot the average number of spikes per direction, the spike counts from individual trials as well as your optimal fit.\n",
    "\n",
    "Select two cells that show nice tuning to test your code.\n",
    "\n",
    "*Grading: 5 pts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vonMises(θ: np.ndarray, α: float, κ: float, ν: float, ϕ: float) -> np.ndarray:\n",
    "    \"\"\"Evaluate the parametric von Mises tuning curve with parameters p at locations theta.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    θ: np.array, shape=(N, )\n",
    "        Locations. The input unit is degree.\n",
    "\n",
    "    α, κ, ν, ϕ : float\n",
    "        Function parameters\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    f: np.array, shape=(N, )\n",
    "        Tuning curve.\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------------\n",
    "    # Implement the Mises model (0.5 pts)\n",
    "    # -----------------------------------\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vonMises(θ: np.ndarray, α: float, κ: float, ν: float, ϕ: float) -> np.ndarray:\n",
    "    \"\"\"Evaluate the parametric von Mises tuning curve \n",
    "        with parameters p at locations theta.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    θ: np.array, shape=(N, )\n",
    "        Locations. The input unit is degree.\n",
    "\n",
    "    α, κ, ν, ϕ : float\n",
    "        Function parameters\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    f: np.array, shape=(N, )\n",
    "        Tuning curve.\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------------\n",
    "    # Implement the Mises model (0.5 pts)\n",
    "    # -----------------------------------\n",
    "    return np.exp(α + κ * (np.cos(2 * (θ - ϕ)) - 1) \\\n",
    "        + ν * (np.cos(θ - ϕ) - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the von Mises function while varying the parameters systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# plot von Mises curves with varying parameters and explain what they do (2 pts)\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spike_count_matrix(counts: np.ndarray, dirs: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute the spike count matrix from the counts and dirs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    counts: np.ndarray\n",
    "        The spike counts for each trial.\n",
    "\n",
    "    dirs: np.ndarray\n",
    "        The stimulus directions for each trial.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    spike_count_matrix: np.ndarray\n",
    "        The computed spike count matrix.\n",
    "    \"\"\"\n",
    "    unique_stim_directions_deg = np.unique(dirs)  # Shape: (nDirs,)\n",
    "    num_unique_directions = len(unique_stim_directions_deg)\n",
    "    logger.debug(f\"Unique stimulus directions: {unique_stim_directions_deg}\")\n",
    "    logger.debug(f\"Number of unique stimulus directions: {num_unique_directions}\")\n",
    " \n",
    "    # Get the unique stimulus directions, sorted. These will be the columns of our matrix.\n",
    "    unique_stim_directions_deg = np.unique(dirs)  # Shape: (nDirs,)\n",
    "    num_unique_directions = len(unique_stim_directions_deg)\n",
    "    # Initialize the spike count matrix `x` with shape (nTrials, nDirs)\n",
    "    # x_jk: j-th trial (row), k-th direction (column)\n",
    "    spike_count_matrix_x = np.zeros((nTrials, num_unique_directions))    \n",
    "    # Populate the matrix\n",
    "    for k_idx, direction_value in enumerate(unique_stim_directions_deg):\n",
    "        # Extract all spike counts from the 1D 'counts' array that correspond to the current 'direction_value'\n",
    "        counts_for_this_direction = counts[dirs == direction_value]\n",
    "        # The get_data function should ensure that 'counts_for_this_direction'\n",
    "        # has exactly 'nTrials' elements.\n",
    "        if len(counts_for_this_direction) == nTrials:\n",
    "            spike_count_matrix_x[:, k_idx] = counts_for_this_direction\n",
    "        else:\n",
    "            # This part handles unexpected lengths, though get_data should prevent this.\n",
    "            # If fewer than nTrials, pad with zeros (already done by get_data, but good for robustness).\n",
    "            # If more than nTrials (unlikely), take the first nTrials.\n",
    "            actual_trials_found = len(counts_for_this_direction)\n",
    "            if actual_trials_found >= nTrials:\n",
    "                spike_count_matrix_x[:, k_idx] = counts_for_this_direction[:nTrials]\n",
    "            else: # actual_trials_found < nTrials\n",
    "                spike_count_matrix_x[:actual_trials_found, k_idx] = counts_for_this_direction\n",
    "                # The remaining (nTrials - actual_trials_found) elements will stay zero \n",
    "                # due to initialization with np.zeros.\n",
    "    return spike_count_matrix_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inital_von_mises_params(mean_counts_to_fit: np.ndarray, unique_dirs_rad: np.ndarray) -> tuple:\n",
    "    \"\"\"Initial guess for the von Mises parameters based on mean counts.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean_counts_to_fit: np.ndarray\n",
    "        The mean counts for each direction.\n",
    "    unique_dirs_rad: np.ndarray\n",
    "        The unique directions in radians.\n",
    "    Returns\n",
    "    -------\n",
    "    tuple: (alpha_guess, kappa_guess, nu_guess, phi_guess_rad)\n",
    "        Initial guesses for the parameters of the von Mises function.\n",
    "    \"\"\"\n",
    "    # Robust initial guesses:\n",
    "    if not np.any(mean_counts_to_fit > 1e-9): # Handle cases where all mean counts are zero or tiny\n",
    "        alpha_guess = np.log(1e-6) # A small baseline\n",
    "        phi_guess_rad = 0.0        # Default preferred direction\n",
    "    else:\n",
    "        # For alpha, use log of mean of positive counts, or log of max if all else fails\n",
    "        positive_mean_counts = mean_counts_to_fit[mean_counts_to_fit > 1e-9]\n",
    "        if len(positive_mean_counts) > 0:\n",
    "            alpha_guess = np.log(np.maximum(1e-6, np.mean(positive_mean_counts)))\n",
    "        else: # Should be caught by the outer if, but as a fallback\n",
    "            alpha_guess = np.log(np.maximum(1e-6, np.max(mean_counts_to_fit)))\n",
    "        \n",
    "        phi_guess_rad = unique_dirs_rad[np.argmax(mean_counts_to_fit)]\n",
    "\n",
    "    kappa_guess = 1.0  # Initial guess for bimodal strength\n",
    "    nu_guess = 1.0     # Initial guess for unimodal strength \n",
    "    return alpha_guess, kappa_guess, nu_guess, phi_guess_rad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuningCurve(counts: np.ndarray, dirs: np.ndarray, show: bool = True) -> np.ndarray:\n",
    "    \"\"\"Fit a von Mises tuning curve to the spike counts in count with direction dir using a least-squares fit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    counts: np.array, shape=(total_n_trials, )\n",
    "        the spike count during the stimulation period\n",
    "\n",
    "    dirs: np.array, shape=(total_n_trials, )\n",
    "        the stimulus direction in degrees\n",
    "\n",
    "    show: bool, default=True\n",
    "        Plot or not.\n",
    "\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    p: np.array or list, (4,)\n",
    "        parameter vector of tuning curve function\n",
    "    \"\"\"\n",
    "    # ----------------------------------------\n",
    "    # Compute the spike count matrix (0.5 pts)\n",
    "    # ----------------------------------------\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # fit the von Mises tuning curve to the spike counts (0.5 pts)\n",
    "    # -----------------------------------------------------------\n",
    "    if show:\n",
    "        # --------------------------------------------\n",
    "        # plot the data and fitted tuning curve (1 pt)\n",
    "        # --------------------------------------------\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuningCurve(counts: np.ndarray, dirs: np.ndarray, show: bool = True) -> np.ndarray:\n",
    "    \"\"\"Fit a von Mises tuning curve to the spike counts in count with \n",
    "    direction dir using a **least-squares fit**.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    counts: np.array, shape=(total_n_trials, )\n",
    "        the spike count during the stimulation period\n",
    "\n",
    "    dirs: np.array, shape=(total_n_trials, )\n",
    "        the stimulus direction in degrees\n",
    "\n",
    "    show: bool, default=True\n",
    "        Plot or not.\n",
    "\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    p: np.array or list, (4,)\n",
    "        parameter vector of tuning curve function\n",
    "    \"\"\"\n",
    "    # ----------------------------------------\n",
    "    # Compute the spike count matrix (0.5 pts)\n",
    "    # ----------------------------------------\n",
    "    logger.info(\"Fitting tuning curve...\")\n",
    "    logger.info(f\"Counts: {counts.shape}\")\n",
    "    logger.info(f\"Dirs: {dirs.shape}\")\n",
    "\n",
    "    spike_count_matrix_x = compute_spike_count_matrix(counts, dirs)\n",
    "    \n",
    "    logger.info(f\"Spike count matrix shape: {spike_count_matrix_x.shape}\") \n",
    "    logger.info(f\"Spike count matrix: {spike_count_matrix_x}\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # fit the von Mises tuning curve to the spike counts (0.5 pts)\n",
    "    # ------------------------------------------------------------\n",
    "\n",
    "\n",
    "    # 1. Calculate mean spike counts per direction\n",
    "    mean_counts_to_fit = np.mean(spike_count_matrix_x, axis=0)\n",
    "    logger.info(f\"Mean counts to fit: {mean_counts_to_fit}\")\n",
    "    \n",
    "    # 2. Get unique directions (degrees) and convert to radians\n",
    "    # 'dirs' is the original 1D array of directions for all trials passed to tuningCurve\n",
    "    unique_stim_directions_deg = np.unique(dirs) \n",
    "    unique_dirs_rad = np.deg2rad(unique_stim_directions_deg)\n",
    "    logger.info(f\"Unique directions (radians) for fitting: {unique_dirs_rad}\")\n",
    "\n",
    "    # Check if there's enough data to fit (at least as many points as parameters)\n",
    "    if len(unique_dirs_rad) < 4: # vonMises has 4 parameters\n",
    "        logger.warning(f\"Not enough unique directions ({len(unique_dirs_rad)}) to fit the von Mises model. Need at least 4. Skipping fit.\")\n",
    "        p_opt = None # Indicate fit failed\n",
    "    else:\n",
    "        alpha_guess, kappa_guess, nu_guess, phi_guess_rad = inital_von_mises_params(mean_counts_to_fit, unique_dirs_rad)\n",
    "        p0 = [alpha_guess, kappa_guess, nu_guess, phi_guess_rad]\n",
    "        logger.info(f\"Initial parameter guesses (p0): {p0}\")\n",
    "        # Bounds: alpha, kappa>=0, nu>=0, phi in [0, 2*pi]\n",
    "        bounds = ([-np.inf, 0, 0, 0], [np.inf, np.inf, np.inf, 2 * np.pi])\n",
    "        # Fit the von Mises function to the mean counts\n",
    "        # 4. Perform the non-linear least squares fit\n",
    "        try:\n",
    "            p_opt, p_cov = opt.curve_fit(\n",
    "                f=vonMises,          # Your vonMises function (make sure it's defined and accessible)\n",
    "                xdata=unique_dirs_rad,\n",
    "                ydata=mean_counts_to_fit,\n",
    "                p0=p0,\n",
    "                bounds=bounds,\n",
    "                maxfev=5000          # Maximum number of function evaluations\n",
    "            )\n",
    "            logger.info(f\"Optimized parameters (p_opt): {p_opt}\")\n",
    "        except RuntimeError:\n",
    "            logger.warning(\"RuntimeError: Optimal parameters not found during curve_fit. Fit failed.\")\n",
    "            p_opt = None # Or assign np.full(4, np.nan) if you prefer NaNs for failed fits\n",
    "        except ValueError as e:\n",
    "            logger.warning(f\"ValueError during curve_fit: {e}. Fit failed.\")\n",
    "            p_opt = None\n",
    "\n",
    " \n",
    "        if show:\n",
    "            # --------------------------------------------\n",
    "            # plot the data and fitted tuning curve (1 pt)\n",
    "            # --------------------------------------------\n",
    "            pass\n",
    "        \n",
    "        return p_opt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot tuning curve and fit for different neurons. Good candidates to check are 28, 29 or 37. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(spikes, neuron):\n",
    "    spk_by_dir = (\n",
    "        spikes[spikes[\"Neuron\"] == neuron]\n",
    "        .groupby([\"Dir\", \"Trial\"])[\"stimPeriod\"]\n",
    "        .sum()\n",
    "        .astype(int)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    dirs = spk_by_dir[\"Dir\"].values\n",
    "    counts = spk_by_dir[\"stimPeriod\"].values\n",
    "\n",
    "    # because we count spikes only when they are present, some zero entries in the count vector are missing\n",
    "    for i, Dir in enumerate(np.unique(spikes[\"Dir\"])):\n",
    "        m = nTrials - np.sum(dirs == Dir)\n",
    "        if m > 0:\n",
    "            dirs = np.concatenate((dirs, np.ones(m) * Dir))\n",
    "            counts = np.concatenate((counts, np.zeros(m)))\n",
    "\n",
    "    idx = np.argsort(dirs)\n",
    "    dirs_sorted = dirs[idx]  # sorted dirs\n",
    "    counts_sorted = counts[idx]\n",
    "\n",
    "    return dirs_sorted, counts_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 10:56:01,506 - INFO - Fitting tuning curve...\n",
      "2025-05-18 10:56:01,506 - INFO - Counts: (176,)\n",
      "2025-05-18 10:56:01,507 - INFO - Dirs: (176,)\n",
      "2025-05-18 10:56:01,509 - INFO - Spike count matrix shape: (11, 16)\n",
      "2025-05-18 10:56:01,511 - INFO - Spike count matrix: [[ 0. 16. 14.  0.  0.  0.  0.  0.  0. 32. 31.  0.  0.  0.  0.  0.]\n",
      " [ 0. 10. 13.  0.  0.  0.  0.  0.  0. 44. 19.  0.  0.  0.  0.  0.]\n",
      " [ 0. 20. 14.  0.  0.  0.  0.  0.  0. 41. 42.  0.  0.  0.  0.  0.]\n",
      " [ 0. 10. 16.  0.  0.  0.  0.  0.  0. 54. 32.  0.  0.  0.  0.  0.]\n",
      " [ 0. 24. 16.  0.  0.  0.  0.  0.  0. 43. 55.  0.  0.  0.  0.  0.]\n",
      " [ 0.  9. 13.  0.  0.  0.  0.  0.  0. 52. 41.  0.  0.  0.  0.  0.]\n",
      " [ 0. 19. 15.  0.  0.  0.  0.  0.  0. 44. 30.  3.  0.  0.  0.  0.]\n",
      " [ 0. 15.  3.  0.  0.  0.  0.  0.  0. 48. 47.  1.  0.  0.  0.  0.]\n",
      " [ 0. 25. 24.  0.  0.  0.  0.  0.  3. 45. 26.  1.  0.  0.  0.  0.]\n",
      " [ 0. 11. 29.  0.  1.  0.  0.  0.  1. 37. 40.  2.  0.  0.  0.  0.]\n",
      " [ 0. 14. 17.  0.  0.  0.  0.  0.  0. 48. 39.  2.  0.  0.  1.  0.]]\n",
      "2025-05-18 10:56:01,512 - INFO - Mean counts to fit: [ 0.         15.72727273 15.81818182  0.          0.09090909  0.\n",
      "  0.          0.          0.36363636 44.36363636 36.54545455  0.81818182\n",
      "  0.          0.          0.09090909  0.        ]\n",
      "2025-05-18 10:56:01,514 - INFO - Unique directions (radians) for fitting: [0.         0.39269908 0.78539816 1.17809725 1.57079633 1.96349541\n",
      " 2.35619449 2.74889357 3.14159265 3.53429174 3.92699082 4.3196899\n",
      " 4.71238898 5.10508806 5.49778714 5.89048623]\n",
      "2025-05-18 10:56:01,515 - INFO - Initial parameter guesses (p0): [2.6551607371818373, 1.0, 1.0, 3.5342917352885173]\n",
      "2025-05-18 10:56:01,529 - INFO - Optimized parameters (p_opt): [4.33282731 8.18559981 0.48435863 3.71739285]\n",
      "2025-05-18 10:56:01,542 - INFO - Fitting tuning curve...\n",
      "2025-05-18 10:56:01,543 - INFO - Counts: (176,)\n",
      "2025-05-18 10:56:01,543 - INFO - Dirs: (176,)\n",
      "2025-05-18 10:56:01,545 - INFO - Spike count matrix shape: (11, 16)\n",
      "2025-05-18 10:56:01,547 - INFO - Spike count matrix: [[ 5.  2.  4. 13.  2.  0.  0.  9.  0.  1. 24.  0.  2.  0.  1.  1.]\n",
      " [ 0.  0. 19. 20.  8.  3.  0.  3.  2.  0.  9.  8.  4.  0.  0.  2.]\n",
      " [ 0.  0.  9. 19. 13.  1.  0.  4.  0.  0.  6.  0. 15.  0.  0.  6.]\n",
      " [ 0.  0.  1. 16.  3.  1.  5.  2.  0.  1.  7. 11.  7.  0.  0.  6.]\n",
      " [ 0.  0. 28. 12.  3.  1. 10.  1.  0.  0.  1.  7.  6.  0.  0.  1.]\n",
      " [ 0.  0. 14. 25.  7.  4.  1.  8.  0.  0.  1.  4.  5.  0.  0.  0.]\n",
      " [ 0.  0. 15. 12.  9.  2.  1.  3.  3.  0. 11.  3.  7.  0.  0.  2.]\n",
      " [ 6.  4. 19. 27.  5.  2.  4.  5.  4.  0. 16.  6.  6.  1.  0.  0.]\n",
      " [ 3.  2. 14. 23.  5.  2.  3.  2.  4.  0. 16.  4.  5.  1.  0.  0.]\n",
      " [ 2.  1. 10. 19. 11.  2.  5.  1.  3.  8.  5.  7.  3.  0.  0.  4.]\n",
      " [ 1.  1. 26. 24.  3.  2.  4.  3.  3.  3.  6.  1. 20.  1.  1.  0.]]\n",
      "2025-05-18 10:56:01,548 - INFO - Mean counts to fit: [ 1.54545455  0.90909091 14.45454545 19.09090909  6.27272727  1.81818182\n",
      "  3.          3.72727273  1.72727273  1.18181818  9.27272727  4.63636364\n",
      "  7.27272727  0.27272727  0.18181818  2.        ]\n",
      "2025-05-18 10:56:01,549 - INFO - Unique directions (radians) for fitting: [0.         0.39269908 0.78539816 1.17809725 1.57079633 1.96349541\n",
      " 2.35619449 2.74889357 3.14159265 3.53429174 3.92699082 4.3196899\n",
      " 4.71238898 5.10508806 5.49778714 5.89048623]\n",
      "2025-05-18 10:56:01,550 - INFO - Initial parameter guesses (p0): [1.5759281335352222, 1.0, 1.0, 1.1780972450961724]\n",
      "2025-05-18 10:56:01,564 - INFO - Optimized parameters (p_opt): [2.96043674 1.90155694 0.41176043 1.06863547]\n",
      "2025-05-18 10:56:01,575 - INFO - Fitting tuning curve...\n",
      "2025-05-18 10:56:01,577 - INFO - Counts: (176,)\n",
      "2025-05-18 10:56:01,578 - INFO - Dirs: (176,)\n",
      "2025-05-18 10:56:01,580 - INFO - Spike count matrix shape: (11, 16)\n",
      "2025-05-18 10:56:01,582 - INFO - Spike count matrix: [[ 10.   4.   1.  82.  66.   0.   0.  15.   5.   0.   0.   3.   6.   2.\n",
      "    0.  11.]\n",
      " [ 11.   9.   0.  84.  62.   0.   0.   8.  12.   0.   0.   9.   4.   0.\n",
      "    2.   5.]\n",
      " [  5.   5.   1.  82.  52.   0.   0.  15.   2.   0.   0.   2.   4.   0.\n",
      "    1.  10.]\n",
      " [  8.   5.   2.  82.  43.   0.   0.   4.   7.   3.   0.  19.   2.   0.\n",
      "    6.  13.]\n",
      " [ 10.   4.   5.  94.  48.   0.   1.   9.   9.   1.   0.   9.   1.   0.\n",
      "    2.   9.]\n",
      " [ 11.   6.   0.  57.  47.   3.   2.   8.  10.   1.   0.   6.  13.   0.\n",
      "    2.  11.]\n",
      " [  5.   3.   2. 109.  47.   2.   3.  10.   6.   5.   0.  10.   2.   0.\n",
      "    3.  10.]\n",
      " [  6.   4.   3.  67.  50.   1.   1.   8.   2.   1.   0.   8.   2.   0.\n",
      "    5.   5.]\n",
      " [ 12.   5.   1.  75.  46.   1.   3.   5.   7.   1.   0.  10.   0.   0.\n",
      "    1.  11.]\n",
      " [ 16.   7.   4.  90.  56.   1.   0.   7.   6.   1.   0.   4.   0.   0.\n",
      "    3.  10.]\n",
      " [  8.   7.   1.  93.  36.   1.   0.  12.  14.   4.   0.   1.   6.   0.\n",
      "    4.  15.]]\n",
      "2025-05-18 10:56:01,583 - INFO - Mean counts to fit: [ 9.27272727  5.36363636  1.81818182 83.18181818 50.27272727  0.81818182\n",
      "  0.90909091  9.18181818  7.27272727  1.54545455  0.          7.36363636\n",
      "  3.63636364  0.18181818  2.63636364 10.        ]\n",
      "2025-05-18 10:56:01,584 - INFO - Unique directions (radians) for fitting: [0.         0.39269908 0.78539816 1.17809725 1.57079633 1.96349541\n",
      " 2.35619449 2.74889357 3.14159265 3.53429174 3.92699082 4.3196899\n",
      " 4.71238898 5.10508806 5.49778714 5.89048623]\n",
      "2025-05-18 10:56:01,585 - INFO - Initial parameter guesses (p0): [2.5569923765609546, 1.0, 1.0, 1.1780972450961724]\n",
      "2025-05-18 10:56:01,599 - INFO - Optimized parameters (p_opt): [4.77770423 7.33355117 1.25860608 1.33130282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron 28: dirs_sorted.shape = (176,), counts_sorted.shape = (176,)\n",
      "Neuron 29: dirs_sorted.shape = (176,), counts_sorted.shape = (176,)\n",
      "Neuron 37: dirs_sorted.shape = (176,), counts_sorted.shape = (176,)\n"
     ]
    }
   ],
   "source": [
    "neurons_to_plot = [28, 29, 37]\n",
    "for neuron in neurons_to_plot:\n",
    "    dirs_sorted, counts_sorted = get_data(spikes, neuron)\n",
    "    result = tuningCurve(counts_sorted, dirs_sorted, show=True)\n",
    "    if len(result) == 4:\n",
    "        print(f\"Neuron {neuron}: dirs_sorted.shape = {dirs_sorted.shape}, counts_sorted.shape = {counts_sorted.shape}\")\n",
    "    else:\n",
    "        print(f\"Neuron {neuron}: No result from tuningCurve()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------\n",
    "# plot the average number of spikes per direction, the spike \n",
    "# counts from individual trials as well as your optimal fit \n",
    "# for different neurons (0.5 pts)\n",
    "# ----------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Permutation test for direction tuning\n",
    "\n",
    "Implement a permutation test to quantitatively assess whether a neuron is direction/orientation selective. To do so, project the vector of average spike counts, $m_k=\\frac{1}{N}\\sum_j x_{jk}$ on a complex exponential with two cycles, $v_k = \\exp(\\psi i \\theta_k)$, where $\\theta_k$ is the $k$-th direction of motion in radians and $\\psi \\in 1,2$ is the fourier component to test (1: direction, 2: orientation). Denote the projection by $q=m^Tv$. The magnitude $|q|$ tells you how much power there is in the $\\psi$-th fourier component. \n",
    "\n",
    "Estimate the distribution of |q| under the null hypothesis that the neuron fires randomly across directions by running 1000 iterations where you repeat the same calculation as above but on a random permutation of the trials (that is, randomly shuffle the entries in the spike count matrix x). The fraction of iterations for which you obtain a value more extreme than what you observed in the data is your p-value. Implement this procedure in the function ```testTuning()```. \n",
    "\n",
    "Illustrate the test procedure for one of the cells from above. Plot the sampling distribution of |q| and indicate the value observed in the real data in your plot. \n",
    "\n",
    "How many cells are tuned at p < 0.01?\n",
    "\n",
    "*Grading: 3 pts*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testTuning(\n",
    "    counts: np.ndarray,\n",
    "    dirs: np.ndarray,\n",
    "    psi: int = 1,\n",
    "    niters: int = 1000,\n",
    "    show: bool = False,\n",
    "    random_seed: int = 2046,\n",
    ") -> Tuple[float, float, np.ndarray]:\n",
    "    \"\"\"Plot the data if show is True, otherwise just return the fit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    counts: np.array, shape=(total_n_trials, )\n",
    "        the spike count during the stimulation period\n",
    "\n",
    "    dirs: np.array, shape=(total_n_trials, )\n",
    "        the stimulus direction in degrees\n",
    "\n",
    "    psi: int\n",
    "        fourier component to test (1 = direction, 2 = orientation)\n",
    "\n",
    "    niters: int\n",
    "        Number of iterations / permutation\n",
    "\n",
    "    show: bool\n",
    "        Plot or not.\n",
    "\n",
    "    random_seed: int\n",
    "        Random seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    p: float\n",
    "        p-value\n",
    "    q: float\n",
    "        magnitude of second Fourier component\n",
    "\n",
    "    qdistr: np.array\n",
    "        sampling distribution of |q| under the null hypothesis\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # -------------------------------\n",
    "    # calculate m, nu and q (0.5 pts)\n",
    "    # -------------------------------\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Estimate the distribution of q under the H0 and obtain the p value (1 pt)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # ensure reproducibility using a random number generator\n",
    "    # hint: access random functions of this generator\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "\n",
    "\n",
    "    if show:\n",
    "        # add plotting code here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_null_distribution(\n",
    "    counts: np.ndarray,\n",
    "    dirs: np.ndarray,\n",
    "    v_k: np.ndarray,\n",
    "    niters: int,\n",
    "   rng: np.random.Generator) -> np.ndarray:\n",
    "    \"\"\"Compute the null distribution of |q| under the null hypothesis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    counts: np.ndarray\n",
    "        The spike counts for each trial.\n",
    "\n",
    "    dirs: np.ndarray\n",
    "        The stimulus directions for each trial.\n",
    "\n",
    "    v_k: np.ndarray\n",
    "        The complex exponential vector for the specified psi.\n",
    "\n",
    "    niters: int\n",
    "        Number of iterations for the permutation test.\n",
    "\n",
    "    rng: np.random.Generator\n",
    "        Random number generator for reproducibility. \n",
    "    Returns\n",
    "    -------\n",
    "    q_distribution_null: np.ndarray\n",
    "        The computed null distribution of |q|.\n",
    "\n",
    "    \"\"\"\n",
    "    # Initialize an array to store the |q| values from each permutation\n",
    "    q_distribution_null = np.zeros(niters)\n",
    "    logger.info(f\"Starting permutation test with {niters} iterations...\")\n",
    "    \n",
    "    # Loop over the number of iterations\n",
    "    for i in range(niters):\n",
    "        # 1. Permute the data: Shuffle the spike counts randomly across all trials.\n",
    "        shuffled_trial_counts = rng.permutation(counts)\n",
    "        \n",
    "        # 2. Recalculate m_k (average spike count per direction) for this permuted dataset.\n",
    "        permuted_spike_matrix = compute_spike_count_matrix(shuffled_trial_counts, dirs)\n",
    "        m_k_permuted = np.mean(permuted_spike_matrix, axis=0)\n",
    "        \n",
    "        # 3. Recalculate |q| using the permuted m_k_permuted and the *original* v_k.\n",
    "        # v_k does not change because it depends on the stimulus directions and psi,\n",
    "        # which are fixed.\n",
    "        q_complex_permuted = np.dot(m_k_permuted, v_k)\n",
    "        q_magnitude_permuted = np.abs(q_complex_permuted)\n",
    "        \n",
    "        # 4. Store the magnitude from this permutation.\n",
    "        q_distribution_null[i] = q_magnitude_permuted\n",
    "        \n",
    "        if (i + 1) % (niters // 10) == 0: # Log progress every 10%\n",
    "            logger.debug(f\"Permutation iteration {i+1}/{niters} completed.\")\n",
    "\n",
    "    logger.info(\"Permutation test finished.\")\n",
    "    \n",
    "    return q_distribution_null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testTuning(\n",
    "    counts: np.ndarray,\n",
    "    dirs: np.ndarray,\n",
    "    psi: int = 1,\n",
    "    niters: int = 1000,\n",
    "    show: bool = False,\n",
    "    random_seed: int = 2046,\n",
    ") -> Tuple[float, float, np.ndarray]:\n",
    "    \"\"\"Plot the data if show is True, otherwise just return the fit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    counts: np.array, shape=(total_n_trials, )\n",
    "        the spike count during the stimulation period\n",
    "\n",
    "    dirs: np.array, shape=(total_n_trials, )\n",
    "        the stimulus direction in degrees\n",
    "\n",
    "    psi: int\n",
    "        fourier component to test (1 = direction, 2 = orientation)\n",
    "\n",
    "    niters: int\n",
    "        Number of iterations / permutation\n",
    "\n",
    "    show: bool\n",
    "        Plot or not.\n",
    "\n",
    "    random_seed: int\n",
    "        Random seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    p: float\n",
    "        p-value\n",
    "    q: float\n",
    "        magnitude of second Fourier component\n",
    "\n",
    "    qdistr: np.array\n",
    "        sampling distribution of |q| under the null hypothesis\n",
    "\n",
    "    \"\"\"\n",
    "    # -------------------------------\n",
    "    # Calculate m, nu and q (0.5 pts)\n",
    "    # -------------------------------\n",
    "    # m - This is the vector of average spike counts for each unique stimulus direction.\n",
    "    spike_count_matrix = compute_spike_count_matrix(counts, dirs) # 'counts' and 'dirs' are 1D arrays of all trials\n",
    "    m_k = np.mean(spike_count_matrix, axis=0)  # Shape: (nUniqueDirs,)\n",
    "\n",
    "    # Get unique directions and convert to radians for v_k\n",
    "    unique_stim_directions_deg = np.unique(dirs) # These are the directions corresponding to columns of spike_count_matrix\n",
    "    theta_k_rad = np.deg2rad(unique_stim_directions_deg) # Shape: (nUniqueDirs,)\n",
    "        \n",
    "    # v_k - This is the complex exponential vector for the specified psi.\n",
    "    # It should be based on the unique radian directions theta_k_rad.\n",
    "    v_k = np.exp(1j * psi * theta_k_rad)  # Corrected: Shape: (nUniqueDirs,)\n",
    "        \n",
    "    # q - This is the projection of m_k onto v_k.\n",
    "    q_complex_observed = np.dot(m_k, v_k)\n",
    "\n",
    "    # Magnitude of the projection for the observed data.\n",
    "    # This is the 'q' to be returned and tested against null distribution.\n",
    "    q_observed_magnitude = np.abs(q_complex_observed) \n",
    "\n",
    "    logger.debug(f\"Observed q magnitude: {q_observed_magnitude}\")\n",
    "    logger.debug(f\"Observed q complex: {q_complex_observed}\")\n",
    "    logger.debug(f\"Observed m_k: {m_k}\")\n",
    "    logger.debug(f\"Observed v_k: {v_k}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Estimate the distribution of q under the H0 and obtain the p value (1 pt)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Ensure reproducibility using a random number generator\n",
    "    # Hint: Access random functions of this generator\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "\n",
    "    # 1-4. Compute the null distribution of |q| under the null hypothesis\n",
    "    #    by running niters iterations where you repeat the same calculation as above\n",
    "    #    but on a random permutation of the trials (that is, randomly shuffle the entries in the spike count matrix x).\n",
    "    #    This is done in the compute_null_distribution function.\n",
    "    #    The q_distribution_null is the array of |q| values from the null distribution.\n",
    "    #    The function compute_null_distribution is defined above.\n",
    "    #    It takes the counts, dirs, v_k, niters, and rng as inputs.\n",
    "    q_distribution_null = compute_null_distribution(counts=counts, dirs=dirs,\n",
    "                                                    v_k=v_k, niters=niters, rng=rng)\n",
    "    \n",
    "    # 5. Calculate the p-value.\n",
    "    #    This is the proportion of permuted |q| values that are as extreme as,\n",
    "    #    or more extreme than, the |q| observed from the original data.\n",
    "    # We use smoothing to avoid p-values of 0 or 1.\n",
    "    p_value = (np.sum(q_distribution_null >= q_observed_magnitude) + 1) / (niters + 1)\n",
    "    # p_value = np.sum(q_distribution_null >= q_observed_magnitude) / niters\n",
    "\n",
    "    # For a slightly more robust p-value, especially if niters is not huge or q_observed_magnitude is very extreme:\n",
    "    # p_value_corrected = (np.sum(q_distribution_null >= q_observed_magnitude) + 1) / (niters + 1)\n",
    "    # You can choose which one to use; the simpler one is fine for this lab typically.\n",
    "    logger.debug(f\"Observed |q|: {q_observed_magnitude:.4f}, Calculated p-value: {p_value:.35f}\")\n",
    "\n",
    "    if show:\n",
    "        # Add plotting code here\n",
    "        pass\n",
    "    \n",
    "    # The array q_distribution_null is the 'qdistr' to be returned.\n",
    "    qdistr = q_distribution_null\n",
    "    return p_value, q_observed_magnitude, qdistr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_neuron_tuning(all_neuron_ids, spikes, niters=1000):\n",
    "    \"\"\"Test the tuning of a neuron for direction and orientation.\"\"\"\n",
    "    tuning_results = []\n",
    "\n",
    "    for i, neuron_id in enumerate(all_neuron_ids):\n",
    "        logger.info(f\"Processing neuron {i+1}/{len(all_neuron_ids)}: ID {neuron_id}...\")\n",
    "\n",
    "        # Get data for the current neuron\n",
    "        dirs_sorted, counts_sorted = get_data(spikes, neuron_id)\n",
    "\n",
    "        # It's possible some neurons might have no spikes in the defined periods.\n",
    "        # get_data should produce empty or all-zero counts_sorted in such cases.\n",
    "        # testTuning should ideally handle this (e.g., result in a non-significant p-value).\n",
    "        # For instance, if all counts are 0, m_k will be 0, q_observed_magnitude will be 0,\n",
    "        # and p_value will likely be 1.0, which is correct (not selective).\n",
    "\n",
    "        # Test for direction tuning (psi=1)\n",
    "        p_direction, q_direction, _ = testTuning(\n",
    "            counts_sorted, \n",
    "            dirs_sorted, \n",
    "            psi=1, \n",
    "            niters=niters, \n",
    "            show=False # No plots for batch processing\n",
    "        )\n",
    "\n",
    "        # Test for orientation tuning (psi=2)\n",
    "        p_orientation, q_orientation, _ = testTuning(\n",
    "            counts_sorted, \n",
    "            dirs_sorted, \n",
    "            psi=2, \n",
    "            niters=niters, \n",
    "            show=False \n",
    "        )\n",
    "\n",
    "        tuning_results.append({\n",
    "            'neuron_id': neuron_id,\n",
    "            'p_direction': p_direction,\n",
    "            'q_direction': q_direction,\n",
    "            'p_orientation': p_orientation,\n",
    "            'q_orientation': q_orientation\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(tuning_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tuning_results(tuning_results_df, alpha_threshold=0.01):\n",
    "    \"\"\"Filter the tuning results based on the significance level.\"\"\"\n",
    "    logging.debug(f\"\\n--- Tuning Selectivity Results (p < {alpha_threshold}) ---\")\n",
    "\n",
    "    direction_selective_neurons = tuning_results_df[tuning_results_df['p_direction'] < alpha_threshold]\n",
    "    orientation_selective_neurons = tuning_results_df[tuning_results_df['p_orientation'] < alpha_threshold]\n",
    "\n",
    "    logging.debug(f\"\\nDirection Selective Neurons (psi=1, p < {alpha_threshold}):\")\n",
    "    if not direction_selective_neurons.empty:\n",
    "        logging.debug(direction_selective_neurons[['neuron_id', 'p_direction', 'q_direction']])\n",
    "    else:\n",
    "        logging.debug(\"No neurons found to be significantly direction selective at this threshold.\")\n",
    "\n",
    "    logging.debug(f\"\\nOrientation Selective Neurons (psi=2, p < {alpha_threshold}):\")\n",
    "    if not orientation_selective_neurons.empty:\n",
    "        logging.debug(orientation_selective_neurons[['neuron_id', 'p_orientation', 'q_orientation']])\n",
    "    else:\n",
    "        logging.debug(\"No neurons found to be significantly orientation selective at this threshold.\")\n",
    "\n",
    "    # You might also be interested in neurons that are BOTH or EXCLUSIVELY one type\n",
    "    both_selective = tuning_results_df[\n",
    "        (tuning_results_df['p_direction'] < alpha_threshold) & \n",
    "        (tuning_results_df['p_orientation'] < alpha_threshold)\n",
    "    ]\n",
    "    logging.debug(f\"\\nNeurons Selective for BOTH Direction and Orientation (p < {alpha_threshold}):\")\n",
    "    if not both_selective.empty:\n",
    "        logging.debug(both_selective[['neuron_id', 'p_direction', 'p_orientation']])\n",
    "    else:\n",
    "        logging.debug(\"No neurons found to be significantly selective for both at this threshold.\")\n",
    "\n",
    "    # Example: Strictly direction selective (significant for direction, not for orientation)\n",
    "    strictly_direction_selective = tuning_results_df[\n",
    "        (tuning_results_df['p_direction'] < alpha_threshold) & \n",
    "        (tuning_results_df['p_orientation'] >= alpha_threshold)\n",
    "    ]\n",
    "    logging.debug(f\"\\nNeurons Strictly Direction Selective (p_dir < {alpha_threshold}, p_ori >= {alpha_threshold}):\")\n",
    "    if not strictly_direction_selective.empty:\n",
    "        logging.debug(strictly_direction_selective[['neuron_id', 'p_direction', 'p_orientation']])\n",
    "    else:\n",
    "        logging.debug(\"No neurons found to be strictly direction selective at this threshold.\")\n",
    "    return (direction_selective_neurons, \n",
    "            orientation_selective_neurons, \n",
    "            both_selective, \n",
    "            strictly_direction_selective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show null distribution for the example cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Plot null distributions for example cells 28 & 29. (1 pt)\n",
    "# ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test all cells for orientation and direction tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 11:15:45,286 - INFO - Processing neuron 1/41: ID 1...\n",
      "2025-05-18 11:15:45,298 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:45,748 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:45,750 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:46,064 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:46,065 - INFO - Processing neuron 2/41: ID 2...\n",
      "2025-05-18 11:15:46,076 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:46,405 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:46,407 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:46,726 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:46,726 - INFO - Processing neuron 3/41: ID 3...\n",
      "2025-05-18 11:15:46,737 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:47,081 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:47,083 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:47,406 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:47,407 - INFO - Processing neuron 4/41: ID 4...\n",
      "2025-05-18 11:15:47,418 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:47,734 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:47,736 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:48,052 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:48,053 - INFO - Processing neuron 5/41: ID 5...\n",
      "2025-05-18 11:15:48,062 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:48,371 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:48,373 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:48,683 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:48,684 - INFO - Processing neuron 6/41: ID 6...\n",
      "2025-05-18 11:15:48,694 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:49,025 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:49,029 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:49,408 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:49,409 - INFO - Processing neuron 7/41: ID 7...\n",
      "2025-05-18 11:15:49,419 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:49,730 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:49,732 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:50,046 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:50,047 - INFO - Processing neuron 8/41: ID 8...\n",
      "2025-05-18 11:15:50,057 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:50,373 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:50,375 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:50,710 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:50,711 - INFO - Processing neuron 9/41: ID 9...\n",
      "2025-05-18 11:15:50,722 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:51,159 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:51,161 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:51,480 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:51,480 - INFO - Processing neuron 10/41: ID 10...\n",
      "2025-05-18 11:15:51,490 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:51,830 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:51,833 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:52,159 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:52,160 - INFO - Processing neuron 11/41: ID 11...\n",
      "2025-05-18 11:15:52,170 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:52,489 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:52,491 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:52,821 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:52,822 - INFO - Processing neuron 12/41: ID 12...\n",
      "2025-05-18 11:15:52,833 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:53,170 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:53,172 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:53,494 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:53,495 - INFO - Processing neuron 13/41: ID 13...\n",
      "2025-05-18 11:15:53,505 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:53,819 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:53,821 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:54,132 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:54,133 - INFO - Processing neuron 14/41: ID 14...\n",
      "2025-05-18 11:15:54,143 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:54,454 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:54,456 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:54,775 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:54,776 - INFO - Processing neuron 15/41: ID 15...\n",
      "2025-05-18 11:15:54,786 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:55,117 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:55,120 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:55,475 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:55,476 - INFO - Processing neuron 16/41: ID 16...\n",
      "2025-05-18 11:15:55,487 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:55,811 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:55,813 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:56,133 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:56,134 - INFO - Processing neuron 17/41: ID 17...\n",
      "2025-05-18 11:15:56,144 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:56,505 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:56,507 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:56,833 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:56,834 - INFO - Processing neuron 18/41: ID 18...\n",
      "2025-05-18 11:15:56,844 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:57,166 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:57,168 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:57,491 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:57,492 - INFO - Processing neuron 19/41: ID 19...\n",
      "2025-05-18 11:15:57,502 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:57,824 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:57,825 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:58,150 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:58,150 - INFO - Processing neuron 20/41: ID 20...\n",
      "2025-05-18 11:15:58,160 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:58,483 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:58,485 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:58,804 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:58,805 - INFO - Processing neuron 21/41: ID 21...\n",
      "2025-05-18 11:15:58,816 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:59,171 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:59,173 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:59,528 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:59,529 - INFO - Processing neuron 22/41: ID 22...\n",
      "2025-05-18 11:15:59,539 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:15:59,889 - INFO - Permutation test finished.\n",
      "2025-05-18 11:15:59,891 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:00,219 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:00,220 - INFO - Processing neuron 23/41: ID 23...\n",
      "2025-05-18 11:16:00,229 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:00,549 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:00,551 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:00,871 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:00,872 - INFO - Processing neuron 24/41: ID 24...\n",
      "2025-05-18 11:16:00,883 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:01,210 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:01,212 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:01,572 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:01,573 - INFO - Processing neuron 25/41: ID 25...\n",
      "2025-05-18 11:16:01,585 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:01,943 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:01,945 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:02,263 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:02,264 - INFO - Processing neuron 26/41: ID 26...\n",
      "2025-05-18 11:16:02,274 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:02,621 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:02,623 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:02,971 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:02,972 - INFO - Processing neuron 27/41: ID 27...\n",
      "2025-05-18 11:16:02,982 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:03,319 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:03,321 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:03,654 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:03,655 - INFO - Processing neuron 28/41: ID 28...\n",
      "2025-05-18 11:16:03,666 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:04,025 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:04,028 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:04,358 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:04,358 - INFO - Processing neuron 29/41: ID 29...\n",
      "2025-05-18 11:16:04,369 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:04,689 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:04,691 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:05,012 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:05,013 - INFO - Processing neuron 30/41: ID 30...\n",
      "2025-05-18 11:16:05,024 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:05,373 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:05,375 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:05,855 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:05,856 - INFO - Processing neuron 31/41: ID 31...\n",
      "2025-05-18 11:16:05,867 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:06,194 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:06,196 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:06,522 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:06,522 - INFO - Processing neuron 32/41: ID 32...\n",
      "2025-05-18 11:16:06,533 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:06,865 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:06,866 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:07,202 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:07,203 - INFO - Processing neuron 33/41: ID 33...\n",
      "2025-05-18 11:16:07,214 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:07,567 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:07,568 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:07,894 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:07,895 - INFO - Processing neuron 34/41: ID 34...\n",
      "2025-05-18 11:16:07,904 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:08,225 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:08,227 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:08,551 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:08,552 - INFO - Processing neuron 35/41: ID 35...\n",
      "2025-05-18 11:16:08,562 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:08,886 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:08,888 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:09,251 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:09,252 - INFO - Processing neuron 36/41: ID 36...\n",
      "2025-05-18 11:16:09,265 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:09,597 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:09,599 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:09,926 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:09,927 - INFO - Processing neuron 37/41: ID 37...\n",
      "2025-05-18 11:16:09,937 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:10,265 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:10,267 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:10,584 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:10,585 - INFO - Processing neuron 38/41: ID 38...\n",
      "2025-05-18 11:16:10,596 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:10,934 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:10,936 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:11,287 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:11,287 - INFO - Processing neuron 39/41: ID 39...\n",
      "2025-05-18 11:16:11,298 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:11,632 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:11,634 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:11,972 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:11,972 - INFO - Processing neuron 40/41: ID 40...\n",
      "2025-05-18 11:16:11,984 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:12,313 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:12,315 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:12,638 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:12,639 - INFO - Processing neuron 41/41: ID 41...\n",
      "2025-05-18 11:16:12,650 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:13,017 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:13,019 - INFO - Starting permutation test with 1000 iterations...\n",
      "2025-05-18 11:16:13,349 - INFO - Permutation test finished.\n",
      "2025-05-18 11:16:13,350 - DEBUG - \n",
      "--- Tuning Selectivity Results (p < 0.01) ---\n",
      "2025-05-18 11:16:13,352 - DEBUG - \n",
      "Direction Selective Neurons (psi=1, p < 0.01):\n",
      "2025-05-18 11:16:13,353 - DEBUG -     neuron_id  p_direction  q_direction\n",
      "12         13     0.000999    77.950288\n",
      "19         20     0.000999     9.297844\n",
      "23         24     0.001998    31.400913\n",
      "24         25     0.003996    35.699993\n",
      "26         27     0.000999    31.640129\n",
      "27         28     0.000999    49.346055\n",
      "28         29     0.002997    20.513794\n",
      "30         31     0.000999    89.125639\n",
      "31         32     0.000999    43.971208\n",
      "36         37     0.000999   124.294759\n",
      "37         38     0.000999   119.906485\n",
      "39         40     0.000999    90.873419\n",
      "2025-05-18 11:16:13,355 - DEBUG - \n",
      "Orientation Selective Neurons (psi=2, p < 0.01):\n",
      "2025-05-18 11:16:13,356 - DEBUG -     neuron_id  p_orientation  q_orientation\n",
      "1           2       0.000999      69.584958\n",
      "2           3       0.001998      27.711406\n",
      "5           6       0.000999      79.765526\n",
      "6           7       0.000999      12.721692\n",
      "7           8       0.000999      81.147705\n",
      "9          10       0.002997      20.855551\n",
      "11         12       0.000999      14.809007\n",
      "12         13       0.000999      48.359041\n",
      "13         14       0.000999      24.945998\n",
      "14         15       0.000999      43.273723\n",
      "15         16       0.000999      42.754483\n",
      "16         17       0.000999      79.649865\n",
      "17         18       0.000999      14.349402\n",
      "19         20       0.000999      12.687638\n",
      "20         21       0.000999     285.435722\n",
      "21         22       0.000999      70.947070\n",
      "22         23       0.000999      11.984867\n",
      "23         24       0.000999      61.437416\n",
      "24         25       0.000999      64.973538\n",
      "25         26       0.000999      27.314082\n",
      "26         27       0.000999      32.030661\n",
      "27         28       0.000999     104.257627\n",
      "28         29       0.000999      40.449320\n",
      "29         30       0.000999     194.989028\n",
      "30         31       0.000999     126.462049\n",
      "31         32       0.000999      47.458766\n",
      "32         33       0.000999      76.786618\n",
      "33         34       0.000999       9.966181\n",
      "35         36       0.000999     191.295301\n",
      "36         37       0.000999      98.977689\n",
      "37         38       0.000999     142.278675\n",
      "38         39       0.001998       7.868793\n",
      "39         40       0.002997      57.588171\n",
      "40         41       0.000999      41.503439\n",
      "2025-05-18 11:16:13,360 - DEBUG - \n",
      "Neurons Selective for BOTH Direction and Orientation (p < 0.01):\n",
      "2025-05-18 11:16:13,361 - DEBUG -     neuron_id  p_direction  p_orientation\n",
      "12         13     0.000999       0.000999\n",
      "19         20     0.000999       0.000999\n",
      "23         24     0.001998       0.000999\n",
      "24         25     0.003996       0.000999\n",
      "26         27     0.000999       0.000999\n",
      "27         28     0.000999       0.000999\n",
      "28         29     0.002997       0.000999\n",
      "30         31     0.000999       0.000999\n",
      "31         32     0.000999       0.000999\n",
      "36         37     0.000999       0.000999\n",
      "37         38     0.000999       0.000999\n",
      "39         40     0.000999       0.002997\n",
      "2025-05-18 11:16:13,365 - DEBUG - \n",
      "Neurons Strictly Direction Selective (p_dir < 0.01, p_ori >= 0.01):\n",
      "2025-05-18 11:16:13,366 - DEBUG - No neurons found to be strictly direction selective at this threshold.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Test all cells for orientation / direction tuning. \n",
    "# Which ones are selective? (0.5 pts)\n",
    "# --------------------------------------------------\n",
    "#%%\n",
    "neuron_tuning_results_df = test_neuron_tuning(np.unique(spikes['Neuron']), spikes, niters=1000)\n",
    "#%% Filter the results based on the significance level\n",
    "direction_selective_neurons, orientation_selective_neurons, both_selective, strictly_direction_selective = \\\n",
    "    filter_tuning_results(neuron_tuning_results_df, alpha_threshold=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of direction tuned neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of direction selective neurons: 12\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of direction selective neurons: {len(direction_selective_neurons)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of orientation tuned neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of orientation selective neurons: 34\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of orientation selective neurons: {len(orientation_selective_neurons)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of neurons selective for both: 12\n",
      "Number of strictly direction selective neurons: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of neurons selective for both: {len(both_selective)}\")\n",
    "print(f\"Number of strictly direction selective neurons: {len(strictly_direction_selective)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tue-summer-2025-oasis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "key": "kernelspec",
     "op": "add",
     "value": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
     }
    }
   ],
   "remote_diff": [
    {
     "key": "kernelspec",
     "op": "add",
     "value": {
      "display_name": "NDS-env",
      "language": "python",
      "name": "python3"
     }
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
